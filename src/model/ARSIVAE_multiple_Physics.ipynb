{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cf845e6",
   "metadata": {},
   "source": [
    "# ARSIVAE DATALOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90be4e8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "import seaborn as sns\n",
    "\n",
    "class CTDataset_ARSIVAE(Dataset):\n",
    "    \"\"\"Dataset with all 14 physics attributes for AR-SI-VAE\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_path, compute_on_fly=True):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.compute_on_fly = compute_on_fly\n",
    "        self.has_precomputed = self._check_precomputed_features()\n",
    "        \n",
    "        if not self.has_precomputed and not compute_on_fly:\n",
    "            raise ValueError(\"CSV missing precomputed features and compute_on_fly=False\")\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"AR-SI-VAE Dataset Loaded\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Total samples: {len(self):,}\")\n",
    "        print(f\"  COVID:  {len(self.df[self.df['label']==1]):,}\")\n",
    "        print(f\"  Normal: {len(self.df[self.df['label']==0]):,}\")\n",
    "        print(f\"Physics: {'On-the-fly' if compute_on_fly else 'Precomputed'}\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    def _check_precomputed_features(self):\n",
    "        required = ['mean_HU', 'HU_std', 'HU_p10', 'HU_p25', 'HU_p50', 'HU_p75', 'HU_p90',\n",
    "                   'mask_area_pixels', 'mask_fraction', 'grad_mean', 'grad_std',\n",
    "                   'contrast', 'homogeneity', 'entropy']\n",
    "        return all(col in self.df.columns for col in required)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def _compute_hu_features(self, ct, mask):\n",
    "        lung_pixels = mask > 0.5\n",
    "        hu_values = ct[lung_pixels]\n",
    "        \n",
    "        if len(hu_values) == 0:\n",
    "            return {k: 0.0 for k in ['mean_HU', 'HU_std', 'HU_p10', 'HU_p25', \n",
    "                                     'HU_p50', 'HU_p75', 'HU_p90']}\n",
    "        \n",
    "        return {\n",
    "            'mean_HU': float(np.mean(hu_values)),\n",
    "            'HU_std': float(np.std(hu_values)),\n",
    "            'HU_p10': float(np.percentile(hu_values, 10)),\n",
    "            'HU_p25': float(np.percentile(hu_values, 25)),\n",
    "            'HU_p50': float(np.percentile(hu_values, 50)),\n",
    "            'HU_p75': float(np.percentile(hu_values, 75)),\n",
    "            'HU_p90': float(np.percentile(hu_values, 90))\n",
    "        }\n",
    "    \n",
    "    def _compute_shape_features(self, mask, image_size=512*512):\n",
    "        mask_area = float(np.sum(mask > 0.5))\n",
    "        return {\n",
    "            'mask_area_pixels': mask_area,\n",
    "            'mask_fraction': mask_area / image_size\n",
    "        }\n",
    "    \n",
    "    def _compute_gradient_features(self, ct, mask):\n",
    "        grad_y, grad_x = np.gradient(ct)\n",
    "        grad_magnitude = np.sqrt(grad_x**2 + grad_y**2)\n",
    "        lung_pixels = mask > 0.5\n",
    "        grad_in_lung = grad_magnitude[lung_pixels]\n",
    "        \n",
    "        if len(grad_in_lung) == 0:\n",
    "            return {'grad_mean': 0.0, 'grad_std': 0.0}\n",
    "        \n",
    "        return {\n",
    "            'grad_mean': float(np.mean(grad_in_lung)),\n",
    "            'grad_std': float(np.std(grad_in_lung))\n",
    "        }\n",
    "    \n",
    "    def _compute_texture_features(self, ct, mask):\n",
    "        lung_pixels = mask > 0.5\n",
    "        if lung_pixels.sum() == 0:\n",
    "            return {'contrast': 0.0, 'homogeneity': 1.0, 'entropy': 0.0}\n",
    "        \n",
    "        ct_masked = ct.copy()\n",
    "        ct_masked[~lung_pixels] = ct_masked[lung_pixels].min()\n",
    "        ct_min = ct_masked[lung_pixels].min()\n",
    "        ct_max = ct_masked[lung_pixels].max()\n",
    "        \n",
    "        if ct_max == ct_min:\n",
    "            return {'contrast': 0.0, 'homogeneity': 1.0, 'entropy': 0.0}\n",
    "        \n",
    "        ct_normalized = ((ct_masked - ct_min) / (ct_max - ct_min) * 255).astype(np.uint8)\n",
    "        \n",
    "        try:\n",
    "            glcm = graycomatrix(ct_normalized, distances=[1], \n",
    "                              angles=[0, np.pi/4, np.pi/2, 3*np.pi/4],\n",
    "                              levels=256, symmetric=True, normed=True)\n",
    "            \n",
    "            contrast = graycoprops(glcm, 'contrast').mean()\n",
    "            homogeneity = graycoprops(glcm, 'homogeneity').mean()\n",
    "            glcm_norm = glcm / (glcm.sum() + 1e-10)\n",
    "            entropy = -np.sum(glcm_norm * np.log2(glcm_norm + 1e-10))\n",
    "            \n",
    "            return {\n",
    "                'contrast': float(contrast),\n",
    "                'homogeneity': float(homogeneity),\n",
    "                'entropy': float(entropy)\n",
    "            }\n",
    "        except:\n",
    "            return {'contrast': 0.0, 'homogeneity': 1.0, 'entropy': 0.0}\n",
    "    \n",
    "    def _compute_all_physics(self, ct, mu, mask):\n",
    "        ct_hu = ct * 700 - 300  # Denormalize to HU\n",
    "        \n",
    "        hu_feat = self._compute_hu_features(ct_hu, mask)\n",
    "        shape_feat = self._compute_shape_features(mask)\n",
    "        grad_feat = self._compute_gradient_features(ct, mask)\n",
    "        texture_feat = self._compute_texture_features(ct, mask)\n",
    "        \n",
    "        attributes = np.array([\n",
    "            hu_feat['mean_HU'], hu_feat['HU_std'], hu_feat['HU_p10'], hu_feat['HU_p25'],\n",
    "            hu_feat['HU_p50'], hu_feat['HU_p75'], hu_feat['HU_p90'],\n",
    "            shape_feat['mask_area_pixels'], shape_feat['mask_fraction'],\n",
    "            grad_feat['grad_mean'], grad_feat['grad_std'],\n",
    "            texture_feat['contrast'], texture_feat['homogeneity'], texture_feat['entropy']\n",
    "        ], dtype=np.float32)\n",
    "        \n",
    "        return attributes\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        ct = np.load(row['ct_path'])\n",
    "        mu = np.load(row['mu_path'])\n",
    "        mask = np.load(row['mask_path'])\n",
    "        \n",
    "        if self.has_precomputed and not self.compute_on_fly:\n",
    "            attributes = np.array([\n",
    "                row['mean_HU'], row['HU_std'], row['HU_p10'], row['HU_p25'],\n",
    "                row['HU_p50'], row['HU_p75'], row['HU_p90'],\n",
    "                row['mask_area_pixels'], row['mask_fraction'],\n",
    "                row['grad_mean'], row['grad_std'],\n",
    "                row['contrast'], row['homogeneity'], row['entropy']\n",
    "            ], dtype=np.float32)\n",
    "        else:\n",
    "            attributes = self._compute_all_physics(ct, mu, mask)\n",
    "        \n",
    "        return {\n",
    "            'ct': torch.FloatTensor(ct).unsqueeze(0),\n",
    "            'mu': torch.FloatTensor(mu).unsqueeze(0),\n",
    "            'mask': torch.FloatTensor(mask).unsqueeze(0),\n",
    "            'attributes': torch.FloatTensor(attributes),\n",
    "            'label': torch.tensor(row['label'], dtype=torch.long),\n",
    "            'id': row['id']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb2e4da",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import r2_score\n",
    "import os\n",
    "import random\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    torch.backends.cudnn.benchmark=False\n",
    "\n",
    "class CTDataset_ARSIVAE(Dataset):\n",
    "    def __init__(self,csv_path,compute_on_fly=True,attr_mean=None,attr_std=None):\n",
    "        self.df=pd.read_csv(csv_path)\n",
    "        self.compute_on_fly=compute_on_fly\n",
    "        self.has_precomputed=self._check_precomputed_features()\n",
    "        self.attr_mean=attr_mean\n",
    "        self.attr_std=attr_std\n",
    "        if not self.has_precomputed and not compute_on_fly:\n",
    "            raise ValueError(\"CSV missing precomputed features\")\n",
    "    def _check_precomputed_features(self):\n",
    "        required=['mean_HU','HU_std','HU_p10','HU_p25','HU_p50','HU_p75','HU_p90','mask_area_pixels','mask_fraction','grad_mean','grad_std','contrast','homogeneity','entropy']\n",
    "        return all(col in self.df.columns for col in required)\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def _compute_hu_features(self,ct,mask):\n",
    "        lung_pixels=mask>0.5\n",
    "        hu_values=ct[lung_pixels]\n",
    "        if len(hu_values)==0:\n",
    "            return {k:0.0 for k in ['mean_HU','HU_std','HU_p10','HU_p25','HU_p50','HU_p75','HU_p90']}\n",
    "        return {'mean_HU':float(np.mean(hu_values)),'HU_std':float(np.std(hu_values)),'HU_p10':float(np.percentile(hu_values,10)),'HU_p25':float(np.percentile(hu_values,25)),'HU_p50':float(np.percentile(hu_values,50)),'HU_p75':float(np.percentile(hu_values,75)),'HU_p90':float(np.percentile(hu_values,90))}\n",
    "    def _compute_shape_features(self,mask,image_size=512*512):\n",
    "        mask_area=float(np.sum(mask>0.5))\n",
    "        return {'mask_area_pixels':mask_area,'mask_fraction':mask_area/image_size}\n",
    "    def _compute_gradient_features(self,ct,mask):\n",
    "        grad_y,grad_x=np.gradient(ct)\n",
    "        grad_magnitude=np.sqrt(grad_x**2+grad_y**2)\n",
    "        lung_pixels=mask>0.5\n",
    "        grad_in_lung=grad_magnitude[lung_pixels]\n",
    "        if len(grad_in_lung)==0:\n",
    "            return {'grad_mean':0.0,'grad_std':0.0}\n",
    "        return {'grad_mean':float(np.mean(grad_in_lung)),'grad_std':float(np.std(grad_in_lung))}\n",
    "    def _compute_texture_features(self,ct,mask):\n",
    "        lung_pixels=mask>0.5\n",
    "        if lung_pixels.sum()==0:\n",
    "            return {'contrast':0.0,'homogeneity':1.0,'entropy':0.0}\n",
    "        ct_masked=ct.copy()\n",
    "        ct_masked[~lung_pixels]=ct_masked[lung_pixels].min()\n",
    "        ct_min=ct_masked[lung_pixels].min()\n",
    "        ct_max=ct_masked[lung_pixels].max()\n",
    "        if ct_max==ct_min:\n",
    "            return {'contrast':0.0,'homogeneity':1.0,'entropy':0.0}\n",
    "        ct_normalized=((ct_masked-ct_min)/(ct_max-ct_min)*255).astype(np.uint8)\n",
    "        try:\n",
    "            glcm=graycomatrix(ct_normalized,distances=[1],angles=[0,np.pi/4,np.pi/2,3*np.pi/4],levels=256,symmetric=True,normed=True)\n",
    "            contrast=graycoprops(glcm,'contrast').mean()\n",
    "            homogeneity=graycoprops(glcm,'homogeneity').mean()\n",
    "            glcm_norm=glcm/(glcm.sum()+1e-10)\n",
    "            entropy=-np.sum(glcm_norm*np.log2(glcm_norm+1e-10))\n",
    "            return {'contrast':float(contrast),'homogeneity':float(homogeneity),'entropy':float(entropy)}\n",
    "        except:\n",
    "            return {'contrast':0.0,'homogeneity':1.0,'entropy':0.0}\n",
    "    def _compute_all_physics(self,ct,mask):\n",
    "        ct_hu=ct*1400-1000\n",
    "        hu_feat=self._compute_hu_features(ct_hu,mask)\n",
    "        shape_feat=self._compute_shape_features(mask)\n",
    "        grad_feat=self._compute_gradient_features(ct,mask)\n",
    "        texture_feat=self._compute_texture_features(ct,mask)\n",
    "        attributes=np.array([hu_feat['mean_HU'],hu_feat['HU_std'],hu_feat['HU_p10'],hu_feat['HU_p25'],hu_feat['HU_p50'],hu_feat['HU_p75'],hu_feat['HU_p90'],shape_feat['mask_area_pixels'],shape_feat['mask_fraction'],grad_feat['grad_mean'],grad_feat['grad_std'],texture_feat['contrast'],texture_feat['homogeneity'],texture_feat['entropy']],dtype=np.float32)\n",
    "        return attributes\n",
    "    def __getitem__(self,idx):\n",
    "        row=self.df.iloc[idx]\n",
    "        ct=np.load(row['ct_path'])\n",
    "        mask=np.load(row['mask_path'])\n",
    "        if self.has_precomputed and not self.compute_on_fly:\n",
    "            attributes=np.array([row['mean_HU'],row['HU_std'],row['HU_p10'],row['HU_p25'],row['HU_p50'],row['HU_p75'],row['HU_p90'],row['mask_area_pixels'],row['mask_fraction'],row['grad_mean'],row['grad_std'],row['contrast'],row['homogeneity'],row['entropy']],dtype=np.float32)\n",
    "        else:\n",
    "            attributes=self._compute_all_physics(ct,mask)\n",
    "        if self.attr_mean is not None and self.attr_std is not None:\n",
    "            attributes=(attributes-self.attr_mean)/(self.attr_std+1e-8)\n",
    "        return {'ct':torch.FloatTensor(ct).unsqueeze(0),'mask':torch.FloatTensor(mask).unsqueeze(0),'attributes':torch.FloatTensor(attributes),'label':torch.tensor(row['label'],dtype=torch.long),'id':row['id']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95a26ff",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,latent_dim=64):\n",
    "        super().__init__()\n",
    "        self.conv=nn.Sequential(nn.Conv2d(1,32,4,2,1),nn.BatchNorm2d(32),nn.LeakyReLU(0.2),nn.Conv2d(32,64,4,2,1),nn.BatchNorm2d(64),nn.LeakyReLU(0.2),nn.Conv2d(64,128,4,2,1),nn.BatchNorm2d(128),nn.LeakyReLU(0.2),nn.Conv2d(128,256,4,2,1),nn.BatchNorm2d(256),nn.LeakyReLU(0.2),nn.Conv2d(256,512,4,2,1),nn.BatchNorm2d(512),nn.LeakyReLU(0.2))\n",
    "        self.fc_mu=nn.Linear(512*16*16,latent_dim)\n",
    "        self.fc_logvar=nn.Linear(512*16*16,latent_dim)\n",
    "        self._init_weights()\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m,(nn.Conv2d,nn.Linear)):\n",
    "                nn.init.kaiming_normal_(m.weight,mode='fan_out',nonlinearity='leaky_relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias,0)\n",
    "        nn.init.xavier_normal_(self.fc_mu.weight,gain=0.01)\n",
    "        nn.init.constant_(self.fc_mu.bias,0)\n",
    "        nn.init.xavier_normal_(self.fc_logvar.weight,gain=0.01)\n",
    "        nn.init.constant_(self.fc_logvar.bias,-5)\n",
    "    def forward(self,x):\n",
    "        h=self.conv(x).view(x.size(0),-1)\n",
    "        mu=self.fc_mu(h)\n",
    "        logvar=torch.clamp(self.fc_logvar(h),-10,2)\n",
    "        return mu,logvar\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,latent_dim=64):\n",
    "        super().__init__()\n",
    "        self.fc=nn.Linear(latent_dim,512*16*16)\n",
    "        self.deconv=nn.Sequential(nn.ConvTranspose2d(512,256,4,2,1),nn.BatchNorm2d(256),nn.LeakyReLU(0.2),nn.ConvTranspose2d(256,128,4,2,1),nn.BatchNorm2d(128),nn.LeakyReLU(0.2),nn.ConvTranspose2d(128,64,4,2,1),nn.BatchNorm2d(64),nn.LeakyReLU(0.2),nn.ConvTranspose2d(64,32,4,2,1),nn.BatchNorm2d(32),nn.LeakyReLU(0.2),nn.ConvTranspose2d(32,1,4,2,1),nn.Tanh())\n",
    "        self._init_weights()\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m,(nn.ConvTranspose2d,nn.Linear)):\n",
    "                nn.init.kaiming_normal_(m.weight,mode='fan_out',nonlinearity='leaky_relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias,0)\n",
    "    def forward(self,z):\n",
    "        h=self.fc(z).view(z.size(0),512,16,16)\n",
    "        return self.deconv(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ff5bc6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class AttributePredictor(nn.Module):\n",
    "    def __init__(self,latent_dim=64,n_attributes=14):\n",
    "        super().__init__()\n",
    "        self.input_layer=nn.Linear(latent_dim,256)\n",
    "        self.bn1=nn.BatchNorm1d(256)\n",
    "        self.res1=nn.Linear(256,256)\n",
    "        self.bn_res1=nn.BatchNorm1d(256)\n",
    "        self.res2=nn.Linear(256,256)\n",
    "        self.bn_res2=nn.BatchNorm1d(256)\n",
    "        self.fc2=nn.Linear(256,128)\n",
    "        self.bn2=nn.BatchNorm1d(128)\n",
    "        self.fc3=nn.Linear(128,n_attributes)\n",
    "        self.dropout=nn.Dropout(0.1)\n",
    "        self._init_weights()\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m,nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight,nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias,0)\n",
    "    def forward(self,z):\n",
    "        x=F.relu(self.bn1(self.input_layer(z)))\n",
    "        identity=x\n",
    "        x=F.relu(self.bn_res1(self.res1(x)))\n",
    "        x=self.bn_res2(self.res2(x))\n",
    "        x=F.relu(x+identity)\n",
    "        x=self.dropout(F.relu(self.bn2(self.fc2(x))))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b68821b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class ARSIVAE(nn.Module):\n",
    "    def __init__(self,latent_dim=64,n_attributes=14):\n",
    "        super().__init__()\n",
    "        self.encoder=Encoder(latent_dim)\n",
    "        self.decoder=Decoder(latent_dim)\n",
    "        self.attr_predictor=AttributePredictor(latent_dim,n_attributes)\n",
    "    def reparameterize(self,mu,logvar):\n",
    "        std=torch.exp(0.5*logvar).clamp(min=1e-4,max=10)\n",
    "        eps=torch.randn_like(std)\n",
    "        return mu+eps*std\n",
    "    def forward(self,x):\n",
    "        mu,logvar=self.encoder(x)\n",
    "        z=self.reparameterize(mu,logvar)\n",
    "        recon=self.decoder(z)\n",
    "        attrs=self.attr_predictor(mu)\n",
    "        return recon,mu,logvar,attrs\n",
    "\n",
    "def loss_function(recon,x,mu,logvar,pred_attrs,true_attrs,beta,lambda_attr):\n",
    "    recon_loss=F.mse_loss(recon,x,reduction='mean')\n",
    "    kl_loss=torch.clamp(-0.5*torch.mean(1+logvar-mu.pow(2)-logvar.exp()),0,1e4)\n",
    "    attr_loss=F.mse_loss(pred_attrs,true_attrs,reduction='mean')\n",
    "    total=recon_loss+beta*kl_loss+lambda_attr*attr_loss\n",
    "    return total,recon_loss,kl_loss,attr_loss\n",
    "\n",
    "def get_improved_schedule(epoch,num_epochs=50):\n",
    "    if epoch<15:\n",
    "        beta=0.001*(epoch/15)\n",
    "        lambda_attr=8.0\n",
    "    elif epoch<35:\n",
    "        progress=(epoch-15)/20\n",
    "        beta=0.001+0.25*progress\n",
    "        lambda_attr=8.0-3.0*progress\n",
    "    else:\n",
    "        beta=0.25\n",
    "        lambda_attr=5.0\n",
    "    return beta,lambda_attr\n",
    "\n",
    "def plot_reconstructions_epoch(model,loader,device,epoch,save_dir='recon_epochs'):\n",
    "    os.makedirs(save_dir,exist_ok=True)\n",
    "    model.eval()\n",
    "    batch=next(iter(loader))\n",
    "    x=batch['ct'][:8].to(device)\n",
    "    with torch.no_grad():\n",
    "        recon,_,_,_=model(x)\n",
    "    x=x.cpu().numpy()\n",
    "    recon=recon.cpu().numpy()\n",
    "    fig,axes=plt.subplots(2,8,figsize=(16,4))\n",
    "    for i in range(8):\n",
    "        axes[0,i].imshow(x[i,0],cmap='gray')\n",
    "        axes[0,i].axis('off')\n",
    "        if i==0:\n",
    "            axes[0,i].set_title('Original')\n",
    "        axes[1,i].imshow(recon[i,0],cmap='gray')\n",
    "        axes[1,i].axis('off')\n",
    "        if i==0:\n",
    "            axes[1,i].set_title('Reconstructed')\n",
    "    plt.suptitle(f'Epoch {epoch}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{save_dir}/recon_epoch_{epoch:03d}.png',dpi=150,bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7904b17",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def train_epoch(model,loader,optimizer,device,beta,lambda_attr):\n",
    "    model.train()\n",
    "    total_loss=recon_loss=kl_loss=attr_loss=0\n",
    "    n_batches=0\n",
    "    pbar=tqdm(loader,desc='Training')\n",
    "    for batch in pbar:\n",
    "        x=batch['ct'].to(device)\n",
    "        attrs=batch['attributes'].to(device)\n",
    "        if torch.isnan(x).any() or torch.isinf(x).any():\n",
    "            continue\n",
    "        optimizer.zero_grad()\n",
    "        recon,mu,logvar,pred_attrs=model(x)\n",
    "        if torch.isnan(recon).any() or torch.isinf(recon).any():\n",
    "            continue\n",
    "        loss,r,k,a=loss_function(recon,x,mu,logvar,pred_attrs,attrs,beta,lambda_attr)\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            continue\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n",
    "        optimizer.step()\n",
    "        total_loss+=loss.item()\n",
    "        recon_loss+=r.item()\n",
    "        kl_loss+=k.item()\n",
    "        attr_loss+=a.item()\n",
    "        n_batches+=1\n",
    "        pbar.set_postfix({'loss':f'{loss.item():.3f}','recon':f'{r.item():.3f}','kl':f'{k.item():.3f}','attr':f'{a.item():.3f}'})\n",
    "    if n_batches==0:\n",
    "        return float('nan'),float('nan'),float('nan'),float('nan')\n",
    "    return total_loss/n_batches,recon_loss/n_batches,kl_loss/n_batches,attr_loss/n_batches\n",
    "\n",
    "def validate(model,loader,device,beta,lambda_attr):\n",
    "    model.eval()\n",
    "    total_loss=recon_loss=kl_loss=attr_loss=0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            x=batch['ct'].to(device)\n",
    "            attrs=batch['attributes'].to(device)\n",
    "            recon,mu,logvar,pred_attrs=model(x)\n",
    "            loss,r,k,a=loss_function(recon,x,mu,logvar,pred_attrs,attrs,beta,lambda_attr)\n",
    "            total_loss+=loss.item()\n",
    "            recon_loss+=r.item()\n",
    "            kl_loss+=k.item()\n",
    "            attr_loss+=a.item()\n",
    "    n=len(loader)\n",
    "    return total_loss/n,recon_loss/n,kl_loss/n,attr_loss/n\n",
    "\n",
    "def train_improved(model,train_loader,val_loader,device,epochs=50):\n",
    "    enc_params=list(model.encoder.parameters())\n",
    "    dec_params=list(model.decoder.parameters())\n",
    "    attr_params=list(model.attr_predictor.parameters())\n",
    "    optimizer=optim.AdamW([{'params':enc_params,'lr':1e-4},{'params':dec_params,'lr':1e-4},{'params':attr_params,'lr':5e-4}],weight_decay=1e-5)\n",
    "    scheduler=optim.lr_scheduler.CosineAnnealingLR(optimizer,epochs)\n",
    "    history={'train_total':[],'val_total':[],'train_recon':[],'val_recon':[],'train_kl':[],'val_kl':[],'train_attr':[],'val_attr':[],'beta':[],'lambda':[]}\n",
    "    best_val_attr_loss=float('inf')\n",
    "    best_epoch=0\n",
    "    for epoch in range(epochs):\n",
    "        beta,lambda_attr=get_improved_schedule(epoch,epochs)\n",
    "        history['beta'].append(beta)\n",
    "        history['lambda'].append(lambda_attr)\n",
    "        train_loss,train_r,train_k,train_a=train_epoch(model,train_loader,optimizer,device,beta,lambda_attr)\n",
    "        val_loss,val_r,val_k,val_a=validate(model,val_loader,device,beta,lambda_attr)\n",
    "        scheduler.step()\n",
    "        history['train_total'].append(train_loss)\n",
    "        history['val_total'].append(val_loss)\n",
    "        history['train_recon'].append(train_r)\n",
    "        history['val_recon'].append(val_r)\n",
    "        history['train_kl'].append(train_k)\n",
    "        history['val_kl'].append(val_k)\n",
    "        history['train_attr'].append(train_a)\n",
    "        history['val_attr'].append(val_a)\n",
    "        phase=\"Physics\" if epoch<15 else \"Balance\" if epoch<35 else \"Finetune\"\n",
    "        print(f\"Epoch {epoch+1}/{epochs} [{phase}] beta={beta:.4f} lambda={lambda_attr:.2f}\")\n",
    "        print(f\"Train: Total={train_loss:.4f} Recon={train_r:.4f} KL={train_k:.4f} Attr={train_a:.4f}\")\n",
    "        print(f\"Val: Total={val_loss:.4f} Recon={val_r:.4f} KL={val_k:.4f} Attr={val_a:.4f}\")\n",
    "        if(epoch+1)%5==0:\n",
    "            plot_reconstructions_epoch(model,val_loader,device,epoch+1)\n",
    "            print(f\"Saved reconstruction for epoch {epoch+1}\")\n",
    "        if val_a<best_val_attr_loss:\n",
    "            best_val_attr_loss=val_a\n",
    "            best_epoch=epoch+1\n",
    "            torch.save(model.state_dict(),'best_arsivae_improved.pth')\n",
    "            print(f\"Best model saved val_attr_loss={val_a:.4f}\")\n",
    "    print(f\"Best model from epoch {best_epoch} with val_attr_loss={best_val_attr_loss:.4f}\")\n",
    "    return model,history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b25bc5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def extract_features(model,loader,device):\n",
    "    model.eval()\n",
    "    latents,labels,pred_attrs,true_attrs=[],[],[],[]\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            x=batch['ct'].to(device)\n",
    "            mu,_=model.encoder(x)\n",
    "            attrs=model.attr_predictor(mu)\n",
    "            latents.append(mu.cpu().numpy())\n",
    "            labels.append(batch['label'].cpu().numpy())\n",
    "            pred_attrs.append(attrs.cpu().numpy())\n",
    "            true_attrs.append(batch['attributes'].cpu().numpy())\n",
    "    return {'latents':np.vstack(latents),'labels':np.concatenate(labels),'pred_attrs':np.vstack(pred_attrs),'true_attrs':np.vstack(true_attrs)}\n",
    "\n",
    "def plot_training_curves(history,save_path='training_curves.png'):\n",
    "    fig,axes=plt.subplots(2,3,figsize=(15,8))\n",
    "    epochs=range(1,len(history['train_total'])+1)\n",
    "    axes[0,0].plot(epochs,history['train_total'],'b-',label='Train')\n",
    "    axes[0,0].plot(epochs,history['val_total'],'r-',label='Val')\n",
    "    axes[0,0].set_title('Total Loss')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(alpha=0.3)\n",
    "    axes[0,1].plot(epochs,history['train_recon'],'b-',label='Train')\n",
    "    axes[0,1].plot(epochs,history['val_recon'],'r-',label='Val')\n",
    "    axes[0,1].set_title('Reconstruction Loss')\n",
    "    axes[0,1].legend()\n",
    "    axes[0,1].grid(alpha=0.3)\n",
    "    axes[0,2].plot(epochs,history['train_kl'],'b-',label='Train')\n",
    "    axes[0,2].plot(epochs,history['val_kl'],'r-',label='Val')\n",
    "    axes[0,2].set_title('KL Divergence')\n",
    "    axes[0,2].legend()\n",
    "    axes[0,2].grid(alpha=0.3)\n",
    "    axes[1,0].plot(epochs,history['train_attr'],'b-',label='Train')\n",
    "    axes[1,0].plot(epochs,history['val_attr'],'r-',label='Val')\n",
    "    axes[1,0].set_title('Attribute Loss')\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].grid(alpha=0.3)\n",
    "    axes[1,1].plot(epochs,history['beta'],'purple')\n",
    "    axes[1,1].set_title('Beta Schedule')\n",
    "    axes[1,1].grid(alpha=0.3)\n",
    "    axes[1,2].plot(epochs,history['lambda'],'orange')\n",
    "    axes[1,2].set_title('Lambda Schedule')\n",
    "    axes[1,2].grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path,dpi=300,bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_physics_alignment(data,save_path='physics_alignment.png'):\n",
    "    pred=data['pred_attrs']\n",
    "    true=data['true_attrs']\n",
    "    attr_names=['mean_HU','HU_std','HU_p10','HU_p25','HU_p50','HU_p75','HU_p90','mask_area','mask_frac','grad_mean','grad_std','contrast','homog','entropy']\n",
    "    fig,axes=plt.subplots(3,5,figsize=(20,12))\n",
    "    axes=axes.flatten()\n",
    "    r2_scores=[]\n",
    "    for i in range(14):\n",
    "        ax=axes[i]\n",
    "        ax.scatter(true[:,i],pred[:,i],alpha=0.3,s=10,color='steelblue')\n",
    "        min_val=min(true[:,i].min(),pred[:,i].min())\n",
    "        max_val=max(true[:,i].max(),pred[:,i].max())\n",
    "        ax.plot([min_val,max_val],[min_val,max_val],'r--')\n",
    "        r2=r2_score(true[:,i],pred[:,i])\n",
    "        r2_scores.append(r2)\n",
    "        ax.set_xlabel(f'True {attr_names[i]}')\n",
    "        ax.set_ylabel(f'Pred {attr_names[i]}')\n",
    "        ax.set_title(f'{attr_names[i]} R2={r2:.3f}')\n",
    "        ax.grid(alpha=0.3)\n",
    "    axes[14].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path,dpi=300,bbox_inches='tight')\n",
    "    plt.close()\n",
    "    return r2_scores,np.mean(r2_scores)\n",
    "\n",
    "def plot_latent_space(data,save_path='latent_space.png'):\n",
    "    latents=data['latents']\n",
    "    labels=data['labels']\n",
    "    pred_attrs=data['pred_attrs']\n",
    "    pca=PCA(n_components=2)\n",
    "    latent_pca=pca.fit_transform(latents)\n",
    "    fig,axes=plt.subplots(2,3,figsize=(18,12))\n",
    "    ax=axes[0,0]\n",
    "    colors=['#3498db','#e74c3c']\n",
    "    for i,label_name in enumerate(['Normal','COVID']):\n",
    "        mask=labels==i\n",
    "        ax.scatter(latent_pca[mask,0],latent_pca[mask,1],c=colors[i],label=label_name,alpha=0.6,s=30)\n",
    "    ax.set_title('Class Separation')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "    physics_features=[('mean_HU',0,'Mean HU'),('grad_mean',9,'Gradient Mean'),('entropy',13,'Entropy'),('mask_area',7,'Mask Area'),('contrast',11,'Contrast')]\n",
    "    for idx,(name,attr_idx,title) in enumerate(physics_features):\n",
    "        ax=axes.flatten()[idx+1]\n",
    "        scatter=ax.scatter(latent_pca[:,0],latent_pca[:,1],c=pred_attrs[:,attr_idx],cmap='viridis',alpha=0.6,s=30)\n",
    "        ax.set_title(f'{title}')\n",
    "        plt.colorbar(scatter,ax=ax,label=name)\n",
    "        ax.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path,dpi=300,bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def compute_normalization_stats(dataset):\n",
    "    all_attrs=[]\n",
    "    for i in tqdm(range(len(dataset)),desc=\"Computing stats\"):\n",
    "        sample=dataset[i]\n",
    "        all_attrs.append(sample['attributes'].numpy())\n",
    "    all_attrs=np.vstack(all_attrs)\n",
    "    mean=all_attrs.mean(axis=0)\n",
    "    std=all_attrs.std(axis=0)\n",
    "    std=np.where(std<1e-6,1.0,std)\n",
    "    return mean,std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b78ff7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    SEED=42\n",
    "    set_seed(SEED)\n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    train_dataset_unnorm=CTDataset_ARSIVAE('train.csv',compute_on_fly=True)\n",
    "    attr_mean,attr_std=compute_normalization_stats(train_dataset_unnorm)\n",
    "    train_dataset=CTDataset_ARSIVAE('train.csv',compute_on_fly=True,attr_mean=attr_mean,attr_std=attr_std)\n",
    "    val_dataset=CTDataset_ARSIVAE('val.csv',compute_on_fly=True,attr_mean=attr_mean,attr_std=attr_std)\n",
    "    test_dataset=CTDataset_ARSIVAE('test.csv',compute_on_fly=True,attr_mean=attr_mean,attr_std=attr_std)\n",
    "    train_loader=DataLoader(train_dataset,batch_size=32,shuffle=True,num_workers=4,pin_memory=True,worker_init_fn=lambda worker_id:np.random.seed(SEED+worker_id))\n",
    "    val_loader=DataLoader(val_dataset,batch_size=32,shuffle=False,num_workers=4,pin_memory=True)\n",
    "    test_loader=DataLoader(test_dataset,batch_size=32,shuffle=False,num_workers=4,pin_memory=True)\n",
    "    model=ARSIVAE(latent_dim=64,n_attributes=14).to(device)\n",
    "    NUM_EPOCHS=50\n",
    "    model,history=train_improved(model,train_loader,val_loader,device,epochs=NUM_EPOCHS)\n",
    "    model.load_state_dict(torch.load('best_arsivae_improved.pth'))\n",
    "    plot_training_curves(history,'training_curves.png')\n",
    "    train_data=extract_features(model,train_loader,device)\n",
    "    val_data=extract_features(model,val_loader,device)\n",
    "    test_data=extract_features(model,test_loader,device)\n",
    "    r2_scores,avg_r2=plot_physics_alignment(val_data,'val_physics_alignment.png')\n",
    "    plot_latent_space(val_data,'val_latent_space.png')\n",
    "    np.save('train_latents.npy',train_data['latents'])\n",
    "    np.save('val_latents.npy',val_data['latents'])\n",
    "    np.save('test_latents.npy',test_data['latents'])\n",
    "    np.save('train_labels.npy',train_data['labels'])\n",
    "    np.save('val_labels.npy',val_data['labels'])\n",
    "    np.save('test_labels.npy',test_data['labels'])\n",
    "    np.save('attr_mean.npy',attr_mean)\n",
    "    np.save('attr_std.npy',attr_std)\n",
    "    print(f\"Training complete. Avg R2={avg_r2:.4f}\")\n",
    "    return model,history,val_data\n",
    "\n",
    "if __name__=='__main__':\n",
    "    model,history,val_data=main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
